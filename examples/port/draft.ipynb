{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d724bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torch.distributions import Multinomial\n",
    "import biom\n",
    "from biom import load_table, Table\n",
    "from biom.util import biom_open\n",
    "\n",
    "from mmvec.util import split_tables, format_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2950e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMVec(nn.Module):\n",
    "    def __init__(self, num_microbes, num_metabolites, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Embedding(num_microbes, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, num_metabolites),\n",
    "            # [batch, sample, metabolite]\n",
    "            nn.Softmax(dim=2)\n",
    "        )\n",
    "        \n",
    "    # X = Batch x num_microbesl\n",
    "    # Y = Batch_size x num_metaboites\n",
    "    def forward(self, X, Y):\n",
    "        z = self.encoder(X)\n",
    "        y_pred = self.decoder(z)\n",
    "#         print(y_pred.shape)\n",
    "#         raise\n",
    "        \n",
    "        # total_count=0 and validate_args=False allows skipping total count when calling log_prob. Were having floating point issues with support.\n",
    "        lp = Multinomial(total_count=0, validate_args=False, probs=y_pred).log_prob(Y).sum(0).sum()\n",
    "        return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ad38d",
   "metadata": {},
   "source": [
    "Getting data prepped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53bc7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually to start\n",
    "microbes = load_table(\"./soil_microbes.biom\")\n",
    "metabolites = load_table(\"./soil_metabolites.biom\")\n",
    "\n",
    "# X = microbes.to_dataframe().T\n",
    "# Y = metabolites.to_dataframe().T\n",
    "# X = X.loc[Y.index]\n",
    "\n",
    "# trainX = X.iloc[:-2]\n",
    "# trainY = Y.iloc[:-2]\n",
    "# testX = X.iloc[-2:]\n",
    "# testY = Y.iloc[-2:]\n",
    "\n",
    "# # index dictionaries for the inputs\n",
    "# microbeIdxs = {microbe: i for i, microbe in enumerate(trainX)}\n",
    "# metaboliteIdxs = {metabolite: i for i, metabolite in enumerate(trainY)}\n",
    "\n",
    "# print(microbeIdxs, \"\\n------\\n\", metaboliteIdxs, \"\\n----\\n\", Y)\n",
    "# exX = torch.tensor(trainX.values, requires_grad=True)\n",
    "# exY = torch.tensor(trainY.values, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8094aa",
   "metadata": {},
   "source": [
    "#### put the data in a dataloader(pytorch iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfac798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicrobeMetaboliteData(Dataset):\n",
    "    def __init__(self, microbes: biom.table, metabolites: biom.table, num_test_samples):\n",
    "        # arrange\n",
    "        self.microbes = microbes.to_dataframe().T   \n",
    "        self.metabolites = metabolites.to_dataframe().T\n",
    "        \n",
    "        # only samples that have results\n",
    "        self.microbes = self.microbes.loc[self.metabolites.index]\n",
    "        \n",
    "        # microbe index\n",
    "#         self.microbe_idxs = {microbe: i for i, microbe in enumerate(self.microbes)}\n",
    "        \n",
    "        # make tensors\n",
    "        self.microbes = torch.tensor(self.microbes.values, dtype=torch.int)\n",
    "        # dtype must be integer to avoid floating point errors in multinomial\n",
    "        self.metabolites = torch.tensor(self.metabolites.values, dtype=torch.int64)\n",
    "        \n",
    "        # inputs in dict so indexs available...not sure if we need this\n",
    "        self.metabolite_idxs = {i: sample for i, sample in enumerate(self.metabolites)}\n",
    "        \n",
    "        self.microbe_count = self.microbes.shape[1]\n",
    "        self.metabolite_count = self.metabolites.shape[1]\n",
    "        \n",
    "        self.metabolites_rel_freq = microbe_relative_frequency = (\n",
    "            self.metabolites.T/self.metabolites.sum(1)).T\n",
    "        \n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.microbes)\n",
    "    \n",
    "#     def __iter__(self):\n",
    "#         # What do we want out?\n",
    "#         # x random feature indexes, where x = the total number of counts in sample\n",
    "#         # the expected outputs for this sample\n",
    "        \n",
    "#         # get the total number of observed features in the sample\n",
    "# #         sample_microbes = self.microbes[idx]\n",
    "# #         sample_microbe_obs = sample_microbes.sum()\n",
    "        \n",
    "#         # generate indexes to feed to the embedding\n",
    "#         relative_frequency = (train_dataset.microbes.T/train_dataset.microbes.sum(1)).T\n",
    "#         batch_multinomial = torch.multinomial(relative_frequency, 50).T\n",
    "       \n",
    "#         return batch_multinomial, self.metabolites\n",
    "\n",
    "def collater(batch_size):\n",
    "    torch.multinomial(microbe_relative_frequency, batch_size, replacement=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa4aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MicrobeMetaboliteData(microbes, metabolites)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "# test_dataset = MicrobeMetaboliteData(testX, testY)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=8,\n",
    "#                                  shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f68a9b",
   "metadata": {},
   "source": [
    "## Training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46b487f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 500\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04e344ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmvec_model = MMVec(train_dataset.microbe_count, train_dataset.metabolite_count, 10)\n",
    "\n",
    "optimizer = torch.optim.Adam(mmvec_model.parameters(), lr=learning_rate, maximize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af146279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -40077.8125\n",
      "Batch #: 0\n",
      "loss: -36404.13671875\n",
      "Batch #: 100\n",
      "loss: -31702.857421875\n",
      "Batch #: 200\n",
      "loss: -27709.734375\n",
      "Batch #: 300\n",
      "loss: -25352.328125\n",
      "Batch #: 400\n",
      "loss: -24233.96875\n",
      "Batch #: 500\n",
      "loss: -23553.99609375\n",
      "Batch #: 600\n",
      "loss: -23262.22265625\n",
      "Batch #: 700\n",
      "loss: -23128.380859375\n",
      "Batch #: 800\n",
      "loss: -23025.640625\n",
      "Batch #: 900\n",
      "loss: -22967.24609375\n",
      "Batch #: 1000\n",
      "loss: -22924.705078125\n",
      "Batch #: 1100\n",
      "loss: -22890.03515625\n",
      "Batch #: 1200\n",
      "loss: -22856.15234375\n",
      "Batch #: 1300\n",
      "loss: -22833.298828125\n",
      "Batch #: 1400\n",
      "loss: -22811.720703125\n",
      "Batch #: 1500\n",
      "loss: -22770.0\n",
      "Batch #: 1600\n",
      "loss: -22737.439453125\n",
      "Batch #: 1700\n",
      "loss: -22731.591796875\n",
      "Batch #: 1800\n",
      "loss: -22710.056640625\n",
      "Batch #: 1900\n",
      "loss: -22683.625\n",
      "Batch #: 2000\n",
      "loss: -22667.99609375\n",
      "Batch #: 2100\n",
      "loss: -22668.8203125\n",
      "Batch #: 2200\n",
      "loss: -22634.103515625\n",
      "Batch #: 2300\n",
      "loss: -22620.904296875\n",
      "Batch #: 2400\n",
      "loss: -22590.16015625\n",
      "Batch #: 2500\n",
      "loss: -22598.8515625\n",
      "Batch #: 2600\n",
      "loss: -22579.01171875\n",
      "Batch #: 2700\n",
      "loss: -22565.62890625\n",
      "Batch #: 2800\n",
      "loss: -22551.29296875\n",
      "Batch #: 2900\n",
      "loss: -22558.0859375\n",
      "Batch #: 3000\n",
      "loss: -22545.619140625\n",
      "Batch #: 3100\n",
      "loss: -22566.419921875\n",
      "Batch #: 3200\n",
      "loss: -22554.974609375\n",
      "Batch #: 3300\n",
      "loss: -22518.55078125\n",
      "Batch #: 3400\n",
      "loss: -22518.5234375\n",
      "Batch #: 3500\n",
      "loss: -22503.546875\n",
      "Batch #: 3600\n",
      "loss: -22511.521484375\n",
      "Batch #: 3700\n",
      "loss: -22496.080078125\n",
      "Batch #: 3800\n",
      "loss: -22492.822265625\n",
      "Batch #: 3900\n",
      "loss: -22501.2734375\n",
      "Batch #: 4000\n",
      "loss: -22491.947265625\n",
      "Batch #: 4100\n",
      "loss: -22498.20703125\n",
      "Batch #: 4200\n",
      "loss: -22476.107421875\n",
      "Batch #: 4300\n",
      "loss: -22460.123046875\n",
      "Batch #: 4400\n",
      "loss: -22467.548828125\n",
      "Batch #: 4500\n",
      "loss: -22461.87890625\n",
      "Batch #: 4600\n",
      "loss: -22469.57421875\n",
      "Batch #: 4700\n",
      "loss: -22470.154296875\n",
      "Batch #: 4800\n",
      "loss: -22472.763671875\n",
      "Batch #: 4900\n",
      "loss: -22477.5625\n",
      "Batch #: 5000\n",
      "loss: -22460.595703125\n",
      "Batch #: 5100\n",
      "loss: -22448.9765625\n",
      "Batch #: 5200\n",
      "loss: -22453.3984375\n",
      "Batch #: 5300\n",
      "loss: -22435.255859375\n",
      "Batch #: 5400\n",
      "loss: -22451.060546875\n",
      "Batch #: 5500\n",
      "loss: -22459.439453125\n",
      "Batch #: 5600\n",
      "loss: -22432.763671875\n",
      "Batch #: 5700\n",
      "loss: -22438.8671875\n",
      "Batch #: 5800\n",
      "loss: -22463.96484375\n",
      "Batch #: 5900\n",
      "loss: -22433.7734375\n",
      "Batch #: 6000\n",
      "loss: -22418.75\n",
      "Batch #: 6100\n",
      "loss: -22413.958984375\n",
      "Batch #: 6200\n",
      "loss: -22434.283203125\n",
      "Batch #: 6300\n",
      "loss: -22435.30859375\n",
      "Batch #: 6400\n",
      "loss: -22440.9609375\n",
      "Batch #: 6500\n",
      "loss: -22399.20703125\n",
      "Batch #: 6600\n",
      "loss: -22430.935546875\n",
      "Batch #: 6700\n",
      "loss: -22419.947265625\n",
      "Batch #: 6800\n",
      "loss: -22411.1484375\n",
      "Batch #: 6900\n",
      "loss: -22415.888671875\n",
      "Batch #: 7000\n",
      "loss: -22413.962890625\n",
      "Batch #: 7100\n",
      "loss: -22429.734375\n",
      "Batch #: 7200\n",
      "loss: -22426.447265625\n",
      "Batch #: 7300\n",
      "loss: -22429.2890625\n",
      "Batch #: 7400\n",
      "loss: -22407.919921875\n",
      "Batch #: 7500\n",
      "loss: -22405.548828125\n",
      "Batch #: 7600\n",
      "loss: -22388.7890625\n",
      "Batch #: 7700\n",
      "loss: -22413.955078125\n",
      "Batch #: 7800\n",
      "loss: -22414.283203125\n",
      "Batch #: 7900\n",
      "loss: -22416.287109375\n",
      "Batch #: 8000\n",
      "loss: -22416.240234375\n",
      "Batch #: 8100\n",
      "loss: -22399.091796875\n",
      "Batch #: 8200\n",
      "loss: -22392.548828125\n",
      "Batch #: 8300\n",
      "loss: -22413.8203125\n",
      "Batch #: 8400\n",
      "loss: -22419.08203125\n",
      "Batch #: 8500\n",
      "loss: -22397.34375\n",
      "Batch #: 8600\n",
      "loss: -22391.75\n",
      "Batch #: 8700\n",
      "loss: -22413.912109375\n",
      "Batch #: 8800\n",
      "loss: -22412.27734375\n",
      "Batch #: 8900\n",
      "loss: -22421.427734375\n",
      "Batch #: 9000\n",
      "loss: -22418.48828125\n",
      "Batch #: 9100\n",
      "loss: -22389.865234375\n",
      "Batch #: 9200\n",
      "loss: -22420.876953125\n",
      "Batch #: 9300\n",
      "loss: -22410.158203125\n",
      "Batch #: 9400\n",
      "loss: -22414.86328125\n",
      "Batch #: 9500\n",
      "loss: -22414.32421875\n",
      "Batch #: 9600\n",
      "loss: -22408.267578125\n",
      "Batch #: 9700\n",
      "loss: -22405.009765625\n",
      "Batch #: 9800\n",
      "loss: -22413.599609375\n",
      "Batch #: 9900\n",
      "loss: -22390.5859375\n",
      "Batch #: 10000\n",
      "loss: -22421.57421875\n",
      "Batch #: 10100\n",
      "loss: -22390.080078125\n",
      "Batch #: 10200\n",
      "loss: -22406.931640625\n",
      "Batch #: 10300\n",
      "loss: -22412.5\n",
      "Batch #: 10400\n",
      "loss: -22409.791015625\n",
      "Batch #: 10500\n",
      "loss: -22409.490234375\n",
      "Batch #: 10600\n",
      "loss: -22393.66796875\n",
      "Batch #: 10700\n",
      "loss: -22387.087890625\n",
      "Batch #: 10800\n",
      "loss: -22397.37109375\n",
      "Batch #: 10900\n",
      "loss: -22394.60546875\n",
      "Batch #: 11000\n",
      "loss: -22380.556640625\n",
      "Batch #: 11100\n",
      "loss: -22409.9375\n",
      "Batch #: 11200\n",
      "loss: -22389.013671875\n",
      "Batch #: 11300\n",
      "loss: -22392.2421875\n",
      "Batch #: 11400\n",
      "loss: -22386.98046875\n",
      "Batch #: 11500\n",
      "loss: -22392.5\n",
      "Batch #: 11600\n",
      "loss: -22397.03125\n",
      "Batch #: 11700\n",
      "loss: -22390.087890625\n",
      "Batch #: 11800\n",
      "loss: -22384.521484375\n",
      "Batch #: 11900\n",
      "loss: -22377.48828125\n",
      "Batch #: 12000\n",
      "loss: -22395.90234375\n",
      "Batch #: 12100\n",
      "loss: -22400.375\n",
      "Batch #: 12200\n",
      "loss: -22386.6015625\n",
      "Batch #: 12300\n",
      "loss: -22382.15234375\n",
      "Batch #: 12400\n",
      "loss: -22380.0234375\n",
      "Batch #: 12500\n",
      "loss: -22382.57421875\n",
      "Batch #: 12600\n",
      "loss: -22400.0625\n",
      "Batch #: 12700\n",
      "loss: -22404.51953125\n",
      "Batch #: 12800\n",
      "loss: -22406.67578125\n",
      "Batch #: 12900\n",
      "loss: -22383.03125\n",
      "Batch #: 13000\n",
      "loss: -22398.955078125\n",
      "Batch #: 13100\n",
      "loss: -22409.3359375\n",
      "Batch #: 13200\n",
      "loss: -22376.529296875\n",
      "Batch #: 13300\n",
      "loss: -22407.970703125\n",
      "Batch #: 13400\n",
      "loss: -22375.927734375\n",
      "Batch #: 13500\n",
      "loss: -22368.158203125\n",
      "Batch #: 13600\n",
      "loss: -22385.193359375\n",
      "Batch #: 13700\n",
      "loss: -22389.529296875\n",
      "Batch #: 13800\n",
      "loss: -22400.671875\n",
      "Batch #: 13900\n",
      "loss: -22399.451171875\n",
      "Batch #: 14000\n",
      "loss: -22382.73828125\n",
      "Batch #: 14100\n",
      "loss: -22391.064453125\n",
      "Batch #: 14200\n",
      "loss: -22381.298828125\n",
      "Batch #: 14300\n",
      "loss: -22381.548828125\n",
      "Batch #: 14400\n",
      "loss: -22381.546875\n",
      "Batch #: 14500\n",
      "loss: -22388.921875\n",
      "Batch #: 14600\n",
      "loss: -22408.2734375\n",
      "Batch #: 14700\n",
      "loss: -22390.158203125\n",
      "Batch #: 14800\n",
      "loss: -22393.345703125\n",
      "Batch #: 14900\n",
      "loss: -22379.63671875\n",
      "Batch #: 15000\n",
      "loss: -22395.27734375\n",
      "Batch #: 15100\n",
      "loss: -22381.677734375\n",
      "Batch #: 15200\n",
      "loss: -22379.33203125\n",
      "Batch #: 15300\n",
      "loss: -22404.31640625\n",
      "Batch #: 15400\n",
      "loss: -22391.330078125\n",
      "Batch #: 15500\n",
      "loss: -22372.818359375\n",
      "Batch #: 15600\n",
      "loss: -22378.416015625\n",
      "Batch #: 15700\n",
      "loss: -22407.26953125\n",
      "Batch #: 15800\n",
      "loss: -22388.91796875\n",
      "Batch #: 15900\n",
      "loss: -22392.08984375\n",
      "Batch #: 16000\n",
      "loss: -22403.9765625\n",
      "Batch #: 16100\n",
      "loss: -22377.71484375\n",
      "Batch #: 16200\n",
      "loss: -22380.5234375\n",
      "Batch #: 16300\n",
      "loss: -22374.494140625\n",
      "Batch #: 16400\n",
      "loss: -22376.447265625\n",
      "Batch #: 16500\n",
      "loss: -22407.01171875\n",
      "Batch #: 16600\n",
      "loss: -22392.15234375\n",
      "Batch #: 16700\n",
      "loss: -22361.89453125\n",
      "Batch #: 16800\n",
      "loss: -22402.900390625\n",
      "Batch #: 16900\n",
      "loss: -22396.8203125\n",
      "Batch #: 17000\n",
      "loss: -22407.607421875\n",
      "Batch #: 17100\n",
      "loss: -22396.439453125\n",
      "Batch #: 17200\n",
      "loss: -22398.61328125\n",
      "Batch #: 17300\n",
      "loss: -22370.76171875\n",
      "Batch #: 17400\n",
      "loss: -22402.607421875\n",
      "Batch #: 17500\n",
      "loss: -22400.115234375\n",
      "Batch #: 17600\n",
      "loss: -22378.23828125\n",
      "Batch #: 17700\n",
      "loss: -22373.453125\n",
      "Batch #: 17800\n",
      "loss: -22379.853515625\n",
      "Batch #: 17900\n",
      "loss: -22390.06640625\n",
      "Batch #: 18000\n",
      "loss: -22394.7109375\n",
      "Batch #: 18100\n",
      "loss: -22385.845703125\n",
      "Batch #: 18200\n",
      "loss: -22358.1640625\n",
      "Batch #: 18300\n",
      "loss: -22382.634765625\n",
      "Batch #: 18400\n",
      "loss: -22392.3671875\n",
      "Batch #: 18500\n",
      "loss: -22369.634765625\n",
      "Batch #: 18600\n",
      "loss: -22398.296875\n",
      "Batch #: 18700\n",
      "loss: -22387.982421875\n",
      "Batch #: 18800\n",
      "loss: -22390.587890625\n",
      "Batch #: 18900\n",
      "loss: -22373.490234375\n",
      "Batch #: 19000\n",
      "loss: -22378.458984375\n",
      "Batch #: 19100\n",
      "loss: -22373.36328125\n",
      "Batch #: 19200\n",
      "loss: -22403.56640625\n",
      "Batch #: 19300\n",
      "loss: -22373.037109375\n",
      "Batch #: 19400\n",
      "loss: -22392.939453125\n",
      "Batch #: 19500\n",
      "loss: -22384.73828125\n",
      "Batch #: 19600\n",
      "loss: -22406.73828125\n",
      "Batch #: 19700\n",
      "loss: -22384.419921875\n",
      "Batch #: 19800\n",
      "loss: -22374.671875\n",
      "Batch #: 19900\n",
      "loss: -22372.201171875\n",
      "Batch #: 20000\n",
      "loss: -22399.123046875\n",
      "Batch #: 20100\n",
      "loss: -22389.67578125\n",
      "Batch #: 20200\n",
      "loss: -22378.748046875\n",
      "Batch #: 20300\n",
      "loss: -22386.80078125\n",
      "Batch #: 20400\n",
      "loss: -22397.52734375\n",
      "Batch #: 20500\n",
      "loss: -22394.009765625\n",
      "Batch #: 20600\n",
      "loss: -22428.361328125\n",
      "Batch #: 20700\n",
      "loss: -22385.662109375\n",
      "Batch #: 20800\n",
      "loss: -22382.529296875\n",
      "Batch #: 20900\n",
      "loss: -22385.8046875\n",
      "Batch #: 21000\n",
      "loss: -22377.68359375\n",
      "Batch #: 21100\n",
      "loss: -22379.365234375\n",
      "Batch #: 21200\n"
     ]
    }
   ],
   "source": [
    "def train_loop(dataset, model, optimizer, batch_size):\n",
    "    n_batches = torch.div(dataset.microbes.sum(),\n",
    "                          batch_size,\n",
    "                          rounding_mode = 'floor') + 1\n",
    "    \n",
    "    microbe_relative_frequency = (dataset.microbes.T/dataset.microbes.sum(1)).T\n",
    "    \n",
    "    for batch in range(n_batches * epochs):\n",
    "    \n",
    "        lp = model(torch.multinomial(microbe_relative_frequency, batch_size, replacement=True).T,\n",
    "              dataset.metabolites_rel_freq)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        lp.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f\"loss: {lp.item()}\\nBatch #: {batch}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "train_loop(train_dataset, mmvec_model, optimizer, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9035dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = mmvec_model(torch.tensor([[0, 0, 0],\n",
    "                                 [1, 1, 1]]), \n",
    "           train_dataset.metabolites[0:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "881a63df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.6305e+10, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95176038",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mout1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m out1[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 0"
     ]
    }
   ],
   "source": [
    "out1[0, 0] + out1[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cc447",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1[0, 1] + out1[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbdc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.microbes.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72287074",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_frequency = (train_dataset.microbes.T/train_dataset.microbes.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(relative_frequency, 50).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_out = mmvec_model(torch.multinomial(relative_frequency, 50).T, train_dataset.metabolites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_out.mean(0).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
