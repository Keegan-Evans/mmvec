{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d54e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.distributions import Multinomial\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa62bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len({i: t for i, t in enumerate(torch.randn((10,3)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d04bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple(Dataset):\n",
    "    def __init__(self):\n",
    "        self.input_data = torch.randn((11, 5))\n",
    "        self.target_values = torch.rand((11, 7))\n",
    "#         self.input_data = {i: t for i, t in enumerate(torch.randn((11, 3)))}\n",
    "#         self.cs = {i: t for i, t in enumerate(torch.rand((11, 5)))}\n",
    "    \n",
    "        print(\"\\n-----\\nmock data: {}\\n\\ntarget output: {}\".format(self.input_data, self.target_values))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx]\n",
    "#         for each in self.input_data:\n",
    "#             return each\n",
    "    \n",
    "#     def __iter__(self):\n",
    "#         for k, v in self.ds:\n",
    "#             print(\"----\\nindex: {}\\nvalue: {}\\n\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13667e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----\n",
      "mock data: tensor([[ 0.6345,  1.1267,  0.6607, -2.3244,  1.9640],\n",
      "        [-1.6958,  0.8349, -0.1122,  0.2532, -0.2475],\n",
      "        [ 0.2809,  0.2856, -0.3635,  0.8085,  0.8682],\n",
      "        [-0.7592,  0.4873, -2.6690,  0.3352, -0.2978],\n",
      "        [-0.7063,  1.2172, -0.4476, -0.6791,  1.9936],\n",
      "        [ 0.8209, -0.0480, -0.6448,  0.1497, -0.1159],\n",
      "        [-0.7147,  0.3699, -0.0973,  0.5046,  0.9169],\n",
      "        [ 0.2818, -0.6481, -1.8644, -1.0849,  0.3173],\n",
      "        [ 0.1218, -0.0676,  0.3210, -0.7297,  0.6859],\n",
      "        [ 0.8857, -0.4955, -0.1660, -0.1517, -1.2053],\n",
      "        [ 1.5045,  0.9623,  0.5579,  0.6380,  0.5878]])\n",
      "\n",
      "target output: tensor([[0.7385, 0.5791, 0.6445, 0.6218, 0.2177, 0.4366, 0.5374],\n",
      "        [0.3247, 0.4145, 0.8585, 0.1303, 0.1534, 0.5820, 0.8935],\n",
      "        [0.2903, 0.2652, 0.3186, 0.0241, 0.9897, 0.2448, 0.5200],\n",
      "        [0.8905, 0.7801, 0.1391, 0.8198, 0.9535, 0.0876, 0.7513],\n",
      "        [0.9876, 0.1603, 0.5984, 0.2344, 0.0389, 0.3260, 0.3178],\n",
      "        [0.3163, 0.1481, 0.4812, 0.8433, 0.2482, 0.1251, 0.4607],\n",
      "        [0.4168, 0.7962, 0.8872, 0.2647, 0.6639, 0.8039, 0.5474],\n",
      "        [0.5439, 0.6782, 0.5108, 0.9301, 0.1505, 0.0539, 0.3641],\n",
      "        [0.3149, 0.6244, 0.6120, 0.7066, 0.5692, 0.4049, 0.5917],\n",
      "        [0.1854, 0.8448, 0.2166, 0.5149, 0.9851, 0.2967, 0.2222],\n",
      "        [0.6419, 0.1600, 0.3384, 0.5838, 0.3017, 0.7126, 0.3962]])\n",
      "tensor([[ 0.6345,  1.1267,  0.6607, -2.3244,  1.9640],\n",
      "        [-1.6958,  0.8349, -0.1122,  0.2532, -0.2475]])\n",
      "tensor([[ 0.2809,  0.2856, -0.3635,  0.8085,  0.8682],\n",
      "        [-0.7592,  0.4873, -2.6690,  0.3352, -0.2978]])\n",
      "tensor([[-0.7063,  1.2172, -0.4476, -0.6791,  1.9936],\n",
      "        [ 0.8209, -0.0480, -0.6448,  0.1497, -0.1159]])\n",
      "tensor([[-0.7147,  0.3699, -0.0973,  0.5046,  0.9169],\n",
      "        [ 0.2818, -0.6481, -1.8644, -1.0849,  0.3173]])\n",
      "tensor([[ 0.1218, -0.0676,  0.3210, -0.7297,  0.6859],\n",
      "        [ 0.8857, -0.4955, -0.1660, -0.1517, -1.2053]])\n",
      "tensor([[1.5045, 0.9623, 0.5579, 0.6380, 0.5878]])\n"
     ]
    }
   ],
   "source": [
    "sim = simple()\n",
    "\n",
    "simple_dataload = DataLoader(sim, batch_size=2, shuffle=False)\n",
    "\n",
    "for thing in simple_dataload:\n",
    "    print(thing)\n",
    "\n",
    "# for i, data, target in iter(simple_dataload):\n",
    "#     print(\"\\n------\\nindexs: {}\\nvalues: {}\".format(i, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b27cd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c73a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand((11, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13544caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 63.,   0., 361.],\n",
      "        [890., 878., 397.],\n",
      "        [711., 723., 315.],\n",
      "        [489., 702., 575.],\n",
      "        [ 30., 435., 578.],\n",
      "        [ 59., 218., 236.],\n",
      "        [680., 732., 198.],\n",
      "        [889., 638., 948.],\n",
      "        [573., 610., 341.],\n",
      "        [214., 478., 450.],\n",
      "        [858., 750., 660.],\n",
      "        [ 64., 532., 219.],\n",
      "        [887., 736., 157.],\n",
      "        [705., 525., 894.],\n",
      "        [478., 837., 546.],\n",
      "        [790.,  48., 306.],\n",
      "        [347., 600., 361.],\n",
      "        [210., 639., 753.],\n",
      "        [844., 954., 261.],\n",
      "        [611.,  45., 707.],\n",
      "        [673.,   9., 207.],\n",
      "        [827., 547., 965.],\n",
      "        [670., 307., 852.],\n",
      "        [413., 579., 167.],\n",
      "        [463., 123., 423.],\n",
      "        [993., 115.,  37.],\n",
      "        [695., 362., 263.],\n",
      "        [666., 772., 779.],\n",
      "        [134., 947., 967.],\n",
      "        [908., 236., 965.],\n",
      "        [391., 310., 582.],\n",
      "        [ 10., 560., 833.],\n",
      "        [629., 344., 825.],\n",
      "        [821., 522., 670.],\n",
      "        [190., 462.,  46.],\n",
      "        [714., 166., 118.],\n",
      "        [879.,  99.,  25.],\n",
      "        [455., 203.,  69.],\n",
      "        [175.,  31., 347.],\n",
      "        [203., 100.,  78.],\n",
      "        [582., 110., 542.],\n",
      "        [780., 238., 390.],\n",
      "        [527., 542., 448.],\n",
      "        [167., 480., 850.],\n",
      "        [805., 560., 258.],\n",
      "        [265., 787., 777.],\n",
      "        [476., 403., 904.],\n",
      "        [913., 282., 175.],\n",
      "        [191., 247., 634.],\n",
      "        [928., 327., 783.]])\n",
      "MAX:y.sum(-1).max())\n",
      "tensor([0.2329, 0.3528, 0.4142])\n",
      "tensor(1.)\n",
      "tensor([ -234.6701,  -312.0637,  -255.6323,   -37.4443,  -184.5166,   -29.8422,\n",
      "         -357.1742,  -118.4883,  -148.7417,   -19.7628,  -142.3381,  -169.3389,\n",
      "         -555.7617,   -84.9104,   -69.2280,  -628.9172,   -63.3359,   -60.1855,\n",
      "         -435.4795,  -459.0013,  -647.4828,  -124.1709,  -182.8401,  -207.6466,\n",
      "         -195.1342, -1072.4189,  -290.3215,   -38.9399,  -219.8949,  -390.3057,\n",
      "          -47.8661,  -339.9768,  -137.2349,  -160.3040,  -241.8720,  -535.0265,\n",
      "         -973.9937,  -306.9180,  -146.6034,   -89.4085,  -297.9308,  -350.9141,\n",
      "          -73.3606,  -106.3718,  -356.2466,   -57.5261,   -75.7026,  -608.8943,\n",
      "          -76.2710,  -313.0859])\n"
     ]
    }
   ],
   "source": [
    "# probs = nn.Softmax(torch.rand((5, 11)))'\n",
    "\n",
    "y = torch.rand((50, 3))\n",
    "\n",
    "y = y * 1000\n",
    "y = y.apply_(int)\n",
    "print(y)\n",
    "sample_probs = nn.Softmax(dim=1)(torch.rand((1, 3))).squeeze()\n",
    "print(\"MAX:y.sum(-1).max())\")\n",
    "print(sample_probs)\n",
    "print(sample_probs.sum())\n",
    "idx = Multinomial(total_count=int(y.sum(-1).max()), probs=sample_probs).log_prob(y)\n",
    "print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f938caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.1941)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Multinomial(probs=torch.tensor([0.9977, 0.5481, 0.3348, 0.5036, 0.7178])).log_prob(torch.tensor([0.2450, 0.2137, 0.1361, 0.2050, 0.2001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581b9118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 11])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "inhomogeneous total_count is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(sim\u001b[38;5;241m.\u001b[39mtarget_values\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m----> 3\u001b[0m \u001b[43mMultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlog_prob()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mmvec_dev/lib/python3.8/site-packages/torch/distributions/multinomial.py:60\u001b[0m, in \u001b[0;36mMultinomial.__init__\u001b[0;34m(self, total_count, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, total_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, validate_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(total_count, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minhomogeneous total_count is not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_count \u001b[38;5;241m=\u001b[39m total_count\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_categorical \u001b[38;5;241m=\u001b[39m Categorical(probs\u001b[38;5;241m=\u001b[39mprobs, logits\u001b[38;5;241m=\u001b[39mlogits)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: inhomogeneous total_count is not supported"
     ]
    }
   ],
   "source": [
    "print(sim.target_values.T.size())\n",
    "\n",
    "Multinomial(nn.Softmax(sim.target_values.T), 11).log_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b207a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Smodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(3, 6)\n",
    "        self.d = nn.Sequential(\n",
    "            nn.Linear(6, 5),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        \n",
    "        #         batch_results = torch.tensor((len(samples), 3))\n",
    "\n",
    "#         for i, sample in enumerate(samples):\n",
    "#             target_idx = torch.randint(0, len(sample))\n",
    "#             batch_results[i, 1] = target_idx\n",
    "    def forward(self, sample_idxs, samples, target):\n",
    "        \n",
    "        z = self.e(target_idx)\n",
    "        print(z)\n",
    "        pred_y = self.d(z)\n",
    "        lp = Multinomial(samples[sample_idxs, target_idx]).log_prob(pred_y).mean()\n",
    "#         pred_y = self.d(z)\n",
    "#         print(\"\\n-----\\nindexes: {}\\npreds: {}\\ntargets: {}\".format(indxs, pred_y, targets))\n",
    "        return batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1039cd45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m simple_model \u001b[38;5;241m=\u001b[39m Smodel()\n\u001b[0;32m----> 3\u001b[0m a, b, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(simple_dataload)\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m      4\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m c[:, :, \u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "simple_model = Smodel()\n",
    "\n",
    "a, b, c = iter(simple_dataload).next()\n",
    "a = a.unsqueeze(1)\n",
    "c[:, :, 3].mean()\n",
    "# first_eval = simple_model(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62899c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c98d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 'embedding' weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.0766,  0.3599, -0.7820,  0.0715,  0.6648],\n",
      "        [-0.2868,  1.6206, -1.5967, -0.0517, -0.3060],\n",
      "        [ 0.2485, -0.2226,  0.9132,  0.2043, -0.0776],\n",
      "        [-1.8054, -0.4760, -0.2929, -0.3481,  0.3487],\n",
      "        [ 0.0371, -0.0677,  0.7737, -0.1092, -0.2712],\n",
      "        [ 0.1416,  0.1295,  0.6814, -0.9583,  0.0639]], requires_grad=True)\n",
      "tensor([[1],\n",
      "        [3]])\n",
      "tensor([[[-0.2868,  1.6206, -1.5967, -0.0517, -0.3060]],\n",
      "\n",
      "        [[-1.8054, -0.4760, -0.2929, -0.3481,  0.3487]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[-0.4651, -1.1347,  0.2816,  0.2815, -1.3331, -0.3286,  0.1355]],\n",
      "\n",
      "        [[ 0.1736, -0.2243,  0.2617,  0.5249, -0.3633, -0.4344, -0.9395]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.3174,  0.1450, -0.6232, -0.0112, -0.3531, -0.8388, -1.2329],\n",
      "        [-0.6806,  0.5161, -0.8823,  0.5617,  1.4444, -0.2090, -3.1117],\n",
      "        [-0.4260,  1.3343,  0.6623,  0.6229,  0.4463,  0.7688,  1.5659],\n",
      "        [ 0.5320, -1.4387,  1.6551,  1.1658,  0.5866,  2.4667, -1.0854],\n",
      "        [ 1.6575, -0.9762, -0.5116, -0.7036, -0.7658, -1.5857,  0.4634],\n",
      "        [ 0.4795,  0.1908, -0.0376, -0.0714, -1.1518,  1.4263,  0.7835]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "\n",
    "alegra = nn.Embedding(6, 5)\n",
    "bart = nn.Linear(5, 7)\n",
    "\n",
    "print(\"the 'embedding' weights:\\n{}\".format(alegra.weight))\n",
    "\n",
    "\n",
    "charlie = torch.tensor([1, 3]).unsqueeze(1)\n",
    "print(charlie)\n",
    "\n",
    "daniel = alegra(charlie)\n",
    "print(daniel)\n",
    "\n",
    "ethan = bart(daniel)\n",
    "print(ethan)\n",
    "\n",
    "# the 'correct' values\n",
    "frank = torch.randn((6, 7))\n",
    "print(frank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46175462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n",
      "tensor([[0.6227],\n",
      "        [0.0157],\n",
      "        [0.7459],\n",
      "        [0.9833],\n",
      "        [0.1442]])\n",
      "Parameter containing:\n",
      "tensor([[-0.0766,  0.3599, -0.7820,  0.0715,  0.6648],\n",
      "        [-0.2868,  1.6206, -1.5967, -0.0517, -0.3060],\n",
      "        [ 0.2485, -0.2226,  0.9132,  0.2043, -0.0776],\n",
      "        [-1.8054, -0.4760, -0.2929, -0.3481,  0.3487],\n",
      "        [ 0.0371, -0.0677,  0.7737, -0.1092, -0.2712],\n",
      "        [ 0.1416,  0.1295,  0.6814, -0.9583,  0.0639]], requires_grad=True)\n",
      "tensor([[1.0221]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gerry = torch.rand((5, 1))\n",
    "isaac = torch.tensor([2])\n",
    "print(isaac)\n",
    "print(gerry)\n",
    "print(alegra.weight)\n",
    "\n",
    "harry = alegra(isaac) @ gerry\n",
    "\n",
    "print(harry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e3d4009",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1604479787.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [15]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(charlie):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff362fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2386,  2.3908, -0.5404,  1.1440,  0.3812],\n",
       "        [-1.8417, -0.5090, -0.4037,  1.9962, -0.2039],\n",
       "        [-0.7219,  1.6073,  0.3807, -0.6203,  0.4096]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_data = torch.randn(36, 1)\n",
    "\n",
    "en = nn.Embedding(36, 5)\n",
    "inds = torch.LongTensor([0, 1, 2])\n",
    "\n",
    "\n",
    "en(inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7e117da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 3.1515, -1.0525,  1.5625],\n",
      "        [ 0.0747, -0.4966, -1.0815],\n",
      "        [ 0.1141, -0.1095,  1.0366],\n",
      "        [ 1.4308, -0.8927,  0.4165],\n",
      "        [ 1.2339,  0.2398, -0.6728]], requires_grad=True)]\n",
      "tensor([[ 0.0747, -0.4966, -1.0815],\n",
      "        [ 0.1141, -0.1095,  1.0366]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 0.0747, -0.4966, -1.0815,  0.1141, -0.1095,  1.0366]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "te = nn.Embedding(5, 3)\n",
    "\n",
    "emidxs = torch.tensor([1, 2])\n",
    "\n",
    "entire = list(te.parameters())\n",
    "\n",
    "vanilla = te(emidxs)\n",
    "\n",
    "fancy = te(emidxs).view((1, -1))\n",
    "\n",
    "print(\"{}\\n{}\\n{}\".format(entire, vanilla, fancy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c07b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "print(ngrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e68f645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: winters  |  context: ['forty', 'When']\n",
      "tensor([ 1, 40])\n",
      "target: shall  |  context: ['winters', 'forty']\n",
      "tensor([87,  1])\n",
      "target: besiege  |  context: ['shall', 'winters']\n",
      "tensor([27, 87])\n",
      "target: thy  |  context: ['besiege', 'shall']\n",
      "tensor([18, 27])\n",
      "target: brow,  |  context: ['thy', 'besiege']\n",
      "tensor([91, 18])\n",
      "target: And  |  context: ['brow,', 'thy']\n",
      "tensor([83, 91])\n",
      "target: dig  |  context: ['And', 'brow,']\n",
      "tensor([60, 83])\n",
      "target: deep  |  context: ['dig', 'And']\n",
      "tensor([61, 60])\n",
      "target: trenches  |  context: ['deep', 'dig']\n",
      "tensor([16, 61])\n",
      "target: in  |  context: ['trenches', 'deep']\n",
      "tensor([86, 16])\n",
      "target: thy  |  context: ['in', 'trenches']\n",
      "tensor([31, 86])\n",
      "target: beauty's  |  context: ['thy', 'in']\n",
      "tensor([91, 31])\n",
      "target: field,  |  context: [\"beauty's\", 'thy']\n",
      "tensor([ 0, 91])\n",
      "target: Thy  |  context: ['field,', \"beauty's\"]\n",
      "tensor([62,  0])\n",
      "target: youth's  |  context: ['Thy', 'field,']\n",
      "tensor([48, 62])\n",
      "target: proud  |  context: [\"youth's\", 'Thy']\n",
      "tensor([20, 48])\n",
      "target: livery  |  context: ['proud', \"youth's\"]\n",
      "tensor([33, 20])\n",
      "target: so  |  context: ['livery', 'proud']\n",
      "tensor([11, 33])\n",
      "target: gazed  |  context: ['so', 'livery']\n",
      "tensor([54, 11])\n",
      "target: on  |  context: ['gazed', 'so']\n",
      "tensor([37, 54])\n",
      "target: now,  |  context: ['on', 'gazed']\n",
      "tensor([78, 37])\n",
      "target: Will  |  context: ['now,', 'on']\n",
      "tensor([59, 78])\n",
      "target: be  |  context: ['Will', 'now,']\n",
      "tensor([15, 59])\n",
      "target: a  |  context: ['be', 'Will']\n",
      "tensor([41, 15])\n",
      "target: totter'd  |  context: ['a', 'be']\n",
      "tensor([43, 41])\n",
      "target: weed  |  context: [\"totter'd\", 'a']\n",
      "tensor([92, 43])\n",
      "target: of  |  context: ['weed', \"totter'd\"]\n",
      "tensor([72, 92])\n",
      "target: small  |  context: ['of', 'weed']\n",
      "tensor([66, 72])\n",
      "target: worth  |  context: ['small', 'of']\n",
      "tensor([69, 66])\n",
      "target: held:  |  context: ['worth', 'small']\n",
      "tensor([24, 69])\n",
      "target: Then  |  context: ['held:', 'worth']\n",
      "tensor([ 7, 24])\n",
      "target: being  |  context: ['Then', 'held:']\n",
      "tensor([56,  7])\n",
      "target: asked,  |  context: ['being', 'Then']\n",
      "tensor([85, 56])\n",
      "target: where  |  context: ['asked,', 'being']\n",
      "tensor([ 3, 85])\n",
      "target: all  |  context: ['where', 'asked,']\n",
      "tensor([47,  3])\n",
      "target: thy  |  context: ['all', 'where']\n",
      "tensor([44, 47])\n",
      "target: beauty  |  context: ['thy', 'all']\n",
      "tensor([91, 44])\n",
      "target: lies,  |  context: ['beauty', 'thy']\n",
      "tensor([71, 91])\n",
      "target: Where  |  context: ['lies,', 'beauty']\n",
      "tensor([77, 71])\n",
      "target: all  |  context: ['Where', 'lies,']\n",
      "tensor([90, 77])\n",
      "target: the  |  context: ['all', 'Where']\n",
      "tensor([44, 90])\n",
      "target: treasure  |  context: ['the', 'all']\n",
      "tensor([82, 44])\n",
      "target: of  |  context: ['treasure', 'the']\n",
      "tensor([ 5, 82])\n",
      "target: thy  |  context: ['of', 'treasure']\n",
      "tensor([66,  5])\n",
      "target: lusty  |  context: ['thy', 'of']\n",
      "tensor([91, 66])\n",
      "target: days;  |  context: ['lusty', 'thy']\n",
      "tensor([28, 91])\n",
      "target: To  |  context: ['days;', 'lusty']\n",
      "tensor([49, 28])\n",
      "target: say,  |  context: ['To', 'days;']\n",
      "tensor([94, 49])\n",
      "target: within  |  context: ['say,', 'To']\n",
      "tensor([ 9, 94])\n",
      "target: thine  |  context: ['within', 'say,']\n",
      "tensor([36,  9])\n",
      "target: own  |  context: ['thine', 'within']\n",
      "tensor([53, 36])\n",
      "target: deep  |  context: ['own', 'thine']\n",
      "tensor([42, 53])\n",
      "target: sunken  |  context: ['deep', 'own']\n",
      "tensor([16, 42])\n",
      "target: eyes,  |  context: ['sunken', 'deep']\n",
      "tensor([30, 16])\n",
      "target: Were  |  context: ['eyes,', 'sunken']\n",
      "tensor([70, 30])\n",
      "target: an  |  context: ['Were', 'eyes,']\n",
      "tensor([35, 70])\n",
      "target: all-eating  |  context: ['an', 'Were']\n",
      "tensor([58, 35])\n",
      "target: shame,  |  context: ['all-eating', 'an']\n",
      "tensor([29, 58])\n",
      "target: and  |  context: ['shame,', 'all-eating']\n",
      "tensor([76, 29])\n",
      "target: thriftless  |  context: ['and', 'shame,']\n",
      "tensor([34, 76])\n",
      "target: praise.  |  context: ['thriftless', 'and']\n",
      "tensor([81, 34])\n",
      "target: How  |  context: ['praise.', 'thriftless']\n",
      "tensor([50, 81])\n",
      "target: much  |  context: ['How', 'praise.']\n",
      "tensor([73, 50])\n",
      "target: more  |  context: ['much', 'How']\n",
      "tensor([96, 73])\n",
      "target: praise  |  context: ['more', 'much']\n",
      "tensor([74, 96])\n",
      "target: deserv'd  |  context: ['praise', 'more']\n",
      "tensor([ 6, 74])\n",
      "target: thy  |  context: [\"deserv'd\", 'praise']\n",
      "tensor([63,  6])\n",
      "target: beauty's  |  context: ['thy', \"deserv'd\"]\n",
      "tensor([91, 63])\n",
      "target: use,  |  context: [\"beauty's\", 'thy']\n",
      "tensor([ 0, 91])\n",
      "target: If  |  context: ['use,', \"beauty's\"]\n",
      "tensor([65,  0])\n",
      "target: thou  |  context: ['If', 'use,']\n",
      "tensor([95, 65])\n",
      "target: couldst  |  context: ['thou', 'If']\n",
      "tensor([ 8, 95])\n",
      "target: answer  |  context: ['couldst', 'thou']\n",
      "tensor([51,  8])\n",
      "target: 'This  |  context: ['answer', 'couldst']\n",
      "tensor([75, 51])\n",
      "target: fair  |  context: [\"'This\", 'answer']\n",
      "tensor([22, 75])\n",
      "target: child  |  context: ['fair', \"'This\"]\n",
      "tensor([67, 22])\n",
      "target: of  |  context: ['child', 'fair']\n",
      "tensor([55, 67])\n",
      "target: mine  |  context: ['of', 'child']\n",
      "tensor([66, 55])\n",
      "target: Shall  |  context: ['mine', 'of']\n",
      "tensor([57, 66])\n",
      "target: sum  |  context: ['Shall', 'mine']\n",
      "tensor([19, 57])\n",
      "target: my  |  context: ['sum', 'Shall']\n",
      "tensor([52, 19])\n",
      "target: count,  |  context: ['my', 'sum']\n",
      "tensor([26, 52])\n",
      "target: and  |  context: ['count,', 'my']\n",
      "tensor([46, 26])\n",
      "target: make  |  context: ['and', 'count,']\n",
      "tensor([34, 46])\n",
      "target: my  |  context: ['make', 'and']\n",
      "tensor([14, 34])\n",
      "target: old  |  context: ['my', 'make']\n",
      "tensor([26, 14])\n",
      "target: excuse,'  |  context: ['old', 'my']\n",
      "tensor([39, 26])\n",
      "target: Proving  |  context: [\"excuse,'\", 'old']\n",
      "tensor([38, 39])\n",
      "target: his  |  context: ['Proving', \"excuse,'\"]\n",
      "tensor([88, 38])\n",
      "target: beauty  |  context: ['his', 'Proving']\n",
      "tensor([17, 88])\n",
      "target: by  |  context: ['beauty', 'his']\n",
      "tensor([71, 17])\n",
      "target: succession  |  context: ['by', 'beauty']\n",
      "tensor([ 2, 71])\n",
      "target: thine!  |  context: ['succession', 'by']\n",
      "tensor([12,  2])\n",
      "target: This  |  context: ['thine!', 'succession']\n",
      "tensor([84, 12])\n",
      "target: were  |  context: ['This', 'thine!']\n",
      "tensor([13, 84])\n",
      "target: to  |  context: ['were', 'This']\n",
      "tensor([21, 13])\n",
      "target: be  |  context: ['to', 'were']\n",
      "tensor([64, 21])\n",
      "target: new  |  context: ['be', 'to']\n",
      "tensor([41, 64])\n",
      "target: made  |  context: ['new', 'be']\n",
      "tensor([ 4, 41])\n",
      "target: when  |  context: ['made', 'new']\n",
      "tensor([89,  4])\n",
      "target: thou  |  context: ['when', 'made']\n",
      "tensor([45, 89])\n",
      "target: art  |  context: ['thou', 'when']\n",
      "tensor([ 8, 45])\n",
      "target: old,  |  context: ['art', 'thou']\n",
      "tensor([32,  8])\n",
      "target: And  |  context: ['old,', 'art']\n",
      "tensor([79, 32])\n",
      "target: see  |  context: ['And', 'old,']\n",
      "tensor([60, 79])\n",
      "target: thy  |  context: ['see', 'And']\n",
      "tensor([23, 60])\n",
      "target: blood  |  context: ['thy', 'see']\n",
      "tensor([91, 23])\n",
      "target: warm  |  context: ['blood', 'thy']\n",
      "tensor([93, 91])\n",
      "target: when  |  context: ['warm', 'blood']\n",
      "tensor([25, 93])\n",
      "target: thou  |  context: ['when', 'warm']\n",
      "tensor([45, 25])\n",
      "target: feel'st  |  context: ['thou', 'when']\n",
      "tensor([ 8, 45])\n",
      "target: it  |  context: [\"feel'st\", 'thou']\n",
      "tensor([68,  8])\n",
      "target: cold.  |  context: ['it', \"feel'st\"]\n",
      "tensor([80, 68])\n"
     ]
    }
   ],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "for context, target in ngrams:\n",
    "    print(\"target: {}  |  context: {}\".format(target, context))\n",
    "    context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "    print(context_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc92cc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 1, 2, 0, 3, 0, 0, 0, 2, 1, 4, 0, 1, 2, 0, 2, 4, 4, 2, 4, 3, 2, 0, 4,\n",
      "         2]]) \n",
      " tensor([[0.4919, 0.4203, 0.2270, 0.5633, 0.8328, 0.8511, 0.3599, 0.0906, 0.2945,\n",
      "         0.3817, 0.5843, 0.7241, 0.6673, 0.3415, 0.0420, 0.7143, 0.6089, 0.1226,\n",
      "         0.5414, 0.0546, 0.1942, 0.7059, 0.7172, 0.8982, 0.1690],\n",
      "        [0.4065, 0.5680, 0.8627, 0.7599, 0.5236, 0.8203, 0.4861, 0.0922, 0.7479,\n",
      "         0.1131, 0.4924, 0.7504, 0.0550, 0.7544, 0.0147, 0.4415, 0.7593, 0.9569,\n",
      "         0.7107, 0.6329, 0.7843, 0.3527, 0.6003, 0.3375, 0.1784],\n",
      "        [0.9599, 0.0402, 0.2174, 0.5971, 0.4090, 0.5508, 0.1647, 0.1346, 0.0786,\n",
      "         0.0070, 0.7698, 0.2942, 0.9401, 0.8595, 0.0551, 0.0698, 0.1540, 0.1370,\n",
      "         0.9220, 0.4467, 0.8543, 0.5019, 0.6808, 0.1251, 0.0417],\n",
      "        [0.9685, 0.0883, 0.9937, 0.1013, 0.9202, 0.4475, 0.5616, 0.4061, 0.9649,\n",
      "         0.4942, 0.7478, 0.4272, 0.8939, 0.4524, 0.8595, 0.5594, 0.6286, 0.0110,\n",
      "         0.5397, 0.0912, 0.2744, 0.2430, 0.9953, 0.1355, 0.6759],\n",
      "        [0.1448, 0.4687, 0.8958, 0.0814, 0.3641, 0.6235, 0.5820, 0.6437, 0.2096,\n",
      "         0.3367, 0.9411, 0.4807, 0.3093, 0.2345, 0.0484, 0.0650, 0.4741, 0.5856,\n",
      "         0.8269, 0.1099, 0.5578, 0.5152, 0.3830, 0.2073, 0.4227],\n",
      "        [0.9924, 0.1017, 0.6733, 0.3456, 0.6483, 0.6620, 0.3720, 0.3679, 0.6138,\n",
      "         0.6668, 0.0379, 0.9110, 0.5448, 0.7480, 0.1376, 0.7513, 0.7686, 0.4587,\n",
      "         0.9836, 0.5680, 0.8076, 0.5247, 0.5980, 0.5903, 0.9304],\n",
      "        [0.7056, 0.2107, 0.5644, 0.7489, 0.0539, 0.2950, 0.6861, 0.3934, 0.3500,\n",
      "         0.2013, 0.7150, 0.8241, 0.9537, 0.0086, 0.8867, 0.1588, 0.9107, 0.5824,\n",
      "         0.5729, 0.4829, 0.1464, 0.8258, 0.2793, 0.9452, 0.5441],\n",
      "        [0.7745, 0.6848, 0.5990, 0.4591, 0.1375, 0.1586, 0.1393, 0.9733, 0.8029,\n",
      "         0.9341, 0.4800, 0.5981, 0.7419, 0.2124, 0.9031, 0.1356, 0.1301, 0.6205,\n",
      "         0.6393, 0.5059, 0.6732, 0.6992, 0.5323, 0.2204, 0.7117],\n",
      "        [0.0243, 0.9540, 0.0316, 0.3949, 0.2723, 0.5233, 0.5832, 0.2665, 0.9213,\n",
      "         0.3272, 0.5584, 0.1699, 0.5219, 0.4894, 0.6482, 0.1765, 0.0765, 0.1296,\n",
      "         0.1478, 0.4431, 0.7958, 0.8857, 0.3167, 0.6933, 0.0131],\n",
      "        [0.8874, 0.4873, 0.4170, 0.6788, 0.8830, 0.0642, 0.8151, 0.9997, 0.8507,\n",
      "         0.6788, 0.8044, 0.3646, 0.1285, 0.6894, 0.8865, 0.0755, 0.1068, 0.3995,\n",
      "         0.2045, 0.6536, 0.0203, 0.8928, 0.0285, 0.3432, 0.0784],\n",
      "        [0.8990, 0.7259, 0.2584, 0.5506, 0.9409, 0.0471, 0.9215, 0.5938, 0.7137,\n",
      "         0.1672, 0.4728, 0.1762, 0.7282, 0.4208, 0.9945, 0.5259, 0.6523, 0.1948,\n",
      "         0.2945, 0.4925, 0.6402, 0.8849, 0.5817, 0.3247, 0.5478],\n",
      "        [0.1779, 0.9956, 0.1643, 0.9848, 0.5439, 0.3081, 0.5683, 0.0565, 0.2900,\n",
      "         0.6914, 0.5320, 0.0488, 0.5071, 0.5174, 0.5769, 0.4449, 0.7283, 0.6126,\n",
      "         0.8907, 0.6636, 0.2007, 0.7344, 0.0120, 0.7602, 0.4382],\n",
      "        [0.4155, 0.6934, 0.3854, 0.5507, 0.0306, 0.2516, 0.7478, 0.7215, 0.3492,\n",
      "         0.0752, 0.9400, 0.3641, 0.8177, 0.9889, 0.7472, 0.6015, 0.4717, 0.2316,\n",
      "         0.7890, 0.5364, 0.2658, 0.9367, 0.5700, 0.2630, 0.8652],\n",
      "        [0.9359, 0.1896, 0.1289, 0.0571, 0.2859, 0.5607, 0.0159, 0.0903, 0.3384,\n",
      "         0.5953, 0.2391, 0.6692, 0.4355, 0.8680, 0.5934, 0.6707, 0.2935, 0.5484,\n",
      "         0.1804, 0.1693, 0.5430, 0.3801, 0.7279, 0.8137, 0.4606],\n",
      "        [0.8733, 0.5086, 0.5737, 0.6233, 0.7238, 0.1126, 0.6178, 0.0921, 0.8737,\n",
      "         0.5974, 0.9364, 0.6987, 0.4812, 0.0708, 0.7509, 0.8630, 0.6046, 0.4076,\n",
      "         0.1085, 0.1101, 0.3120, 0.9357, 0.1812, 0.2105, 0.9023],\n",
      "        [0.7352, 0.4180, 0.8762, 0.5892, 0.0569, 0.9384, 0.9259, 0.5682, 0.0173,\n",
      "         0.0936, 0.6646, 0.6191, 0.3396, 0.5639, 0.9101, 0.2521, 0.5099, 0.2433,\n",
      "         0.1873, 0.7874, 0.5124, 0.6661, 0.6544, 0.5274, 0.1864],\n",
      "        [0.6283, 0.0047, 0.5078, 0.9231, 0.0496, 0.3842, 0.9339, 0.5824, 0.4733,\n",
      "         0.6284, 0.5083, 0.6487, 0.0615, 0.4384, 0.7836, 0.0088, 0.6054, 0.9108,\n",
      "         0.4328, 0.0137, 0.2245, 0.7544, 0.2129, 0.3484, 0.6513],\n",
      "        [0.2619, 0.3227, 0.8797, 0.6185, 0.6764, 0.2336, 0.2755, 0.0814, 0.1633,\n",
      "         0.8490, 0.7938, 0.7670, 0.7436, 0.2199, 0.8553, 0.5565, 0.6931, 0.8858,\n",
      "         0.6435, 0.8120, 0.8785, 0.4953, 0.9284, 0.6188, 0.7539],\n",
      "        [0.6099, 0.7710, 0.8821, 0.8560, 0.6941, 0.1299, 0.9505, 0.6614, 0.6996,\n",
      "         0.4960, 0.2771, 0.5074, 0.3318, 0.8467, 0.4924, 0.7566, 0.0148, 0.4524,\n",
      "         0.2219, 0.9122, 0.7681, 0.1702, 0.8537, 0.3785, 0.1486],\n",
      "        [0.1890, 0.4590, 0.1834, 0.5127, 0.3909, 0.4556, 0.6539, 0.7000, 0.0815,\n",
      "         0.2086, 0.6518, 0.8485, 0.5346, 0.6612, 0.2690, 0.6176, 0.9303, 0.8755,\n",
      "         0.9812, 0.8536, 0.0303, 0.7106, 0.5128, 0.9775, 0.8324],\n",
      "        [0.2388, 0.3827, 0.8727, 0.2676, 0.6810, 0.0939, 0.6291, 0.8769, 0.4441,\n",
      "         0.0732, 0.3908, 0.2341, 0.6415, 0.3680, 0.9282, 0.7182, 0.5306, 0.6063,\n",
      "         0.1056, 0.9015, 0.8658, 0.5890, 0.9548, 0.9541, 0.0506],\n",
      "        [0.1125, 0.1038, 0.2975, 0.7656, 0.2722, 0.8723, 0.0793, 0.2508, 0.8581,\n",
      "         0.2947, 0.1431, 0.4588, 0.9156, 0.2239, 0.7710, 0.3809, 0.4235, 0.6574,\n",
      "         0.4721, 0.0702, 0.2793, 0.2743, 0.6980, 0.0584, 0.7133],\n",
      "        [0.4689, 0.4803, 0.6052, 0.7784, 0.0175, 0.0676, 0.6597, 0.7716, 0.7563,\n",
      "         0.2761, 0.3069, 0.4334, 0.9750, 0.2860, 0.6022, 0.3820, 0.6292, 0.2644,\n",
      "         0.4118, 0.8132, 0.3061, 0.4585, 0.2103, 0.0312, 0.4940],\n",
      "        [0.8851, 0.9310, 0.6553, 0.7706, 0.6560, 0.0419, 0.5257, 0.7046, 0.1369,\n",
      "         0.2495, 0.2493, 0.4045, 0.5012, 0.1468, 0.6570, 0.5832, 0.8718, 0.2262,\n",
      "         0.4458, 0.8740, 0.8548, 0.3240, 0.3023, 0.1328, 0.4623],\n",
      "        [0.9043, 0.1258, 0.4937, 0.9997, 0.8660, 0.0405, 0.9147, 0.8523, 0.9055,\n",
      "         0.3671, 0.6700, 0.6978, 0.3117, 0.5756, 0.7466, 0.8590, 0.9387, 0.2398,\n",
      "         0.4198, 0.6112, 0.5861, 0.3208, 0.8026, 0.1210, 0.1330]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 25])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(5, (1, 25))\n",
    "b = torch.rand(25, 25)\n",
    "\n",
    "print(a, \"\\n\", b)\n",
    "c = a * b\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15c625da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8],\n",
       "        [3]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor((torch.tensor(8), torch.tensor(3))).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9439e7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 2: 3, 4: 5}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = {}\n",
    "g = { i: cnt.item() for i, cnt in enumerate(torch.tensor([0, 2, 3, 0, 5])) if cnt > 0}\n",
    "# print(g, \"\\n\")\n",
    "# g[:, 0]\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4b46eb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 2, 0, 1, 1, 2, 1, 2, 0, 2, 1, 2]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = 3\n",
    "cnt = 15\n",
    "\n",
    "torch.randint(unique, (1, cnt)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a5f649f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rand_size(Dataset):\n",
    "    def __init__(self):\n",
    "        self.a = torch.tensor([[0, 1, 2, 0, 0],\n",
    "                               [3, 0, 0, 1, 2]])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.a)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.a[idx][self.a[idx] != 0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc6e61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = rand_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a5e127c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = DataLoader(rs, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3338d7f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 1] at entry 0 and [3, 1] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mmvec_dev/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mmvec_dev/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mmvec_dev/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mmvec_dev/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:138\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    136\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel)\n\u001b[1;32m    137\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 1] at entry 0 and [3, 1] at entry 1"
     ]
    }
   ],
   "source": [
    "next(iter(rl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84a522fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = torch.tensor([0, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37e0bdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb[tb != 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
