{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d54e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d15f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ds = {i: t for i, t in enumerate(torch.randn((10,3)))}\n",
    "    \n",
    "    def __len__(self):\n",
    "        len(self.ds)\n",
    "    def __iter__(self):\n",
    "        for k, v in self.ds:\n",
    "            print(\"----\\nindex: {}\\nvalue: {}\\n\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8d1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c98d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 'embedding' weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.0766,  0.3599, -0.7820,  0.0715,  0.6648],\n",
      "        [-0.2868,  1.6206, -1.5967, -0.0517, -0.3060],\n",
      "        [ 0.2485, -0.2226,  0.9132,  0.2043, -0.0776],\n",
      "        [-1.8054, -0.4760, -0.2929, -0.3481,  0.3487],\n",
      "        [ 0.0371, -0.0677,  0.7737, -0.1092, -0.2712],\n",
      "        [ 0.1416,  0.1295,  0.6814, -0.9583,  0.0639]], requires_grad=True)\n",
      "tensor([[1],\n",
      "        [3]])\n",
      "tensor([[[-0.2868,  1.6206, -1.5967, -0.0517, -0.3060]],\n",
      "\n",
      "        [[-1.8054, -0.4760, -0.2929, -0.3481,  0.3487]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[-0.4651, -1.1347,  0.2816,  0.2815, -1.3331, -0.3286,  0.1355]],\n",
      "\n",
      "        [[ 0.1736, -0.2243,  0.2617,  0.5249, -0.3633, -0.4344, -0.9395]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.3174,  0.1450, -0.6232, -0.0112, -0.3531, -0.8388, -1.2329],\n",
      "        [-0.6806,  0.5161, -0.8823,  0.5617,  1.4444, -0.2090, -3.1117],\n",
      "        [-0.4260,  1.3343,  0.6623,  0.6229,  0.4463,  0.7688,  1.5659],\n",
      "        [ 0.5320, -1.4387,  1.6551,  1.1658,  0.5866,  2.4667, -1.0854],\n",
      "        [ 1.6575, -0.9762, -0.5116, -0.7036, -0.7658, -1.5857,  0.4634],\n",
      "        [ 0.4795,  0.1908, -0.0376, -0.0714, -1.1518,  1.4263,  0.7835]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "\n",
    "alegra = nn.Embedding(6, 5)\n",
    "bart = nn.Linear(5, 7)\n",
    "\n",
    "print(\"the 'embedding' weights:\\n{}\".format(alegra.weight))\n",
    "\n",
    "\n",
    "charlie = torch.tensor([1, 3]).unsqueeze(1)\n",
    "print(charlie)\n",
    "\n",
    "daniel = alegra(charlie)\n",
    "print(daniel)\n",
    "\n",
    "ethan = bart(daniel)\n",
    "print(ethan)\n",
    "\n",
    "# the 'correct' values\n",
    "frank = torch.randn((6, 7))\n",
    "print(frank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7c3e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "index: 0\n",
      "value: tensor([1])\n",
      "\n",
      "----\n",
      "index: 1\n",
      "value: tensor([3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(charlie):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff362fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 4.9849e-01,  1.7822e+00,  3.5531e-01,  6.6323e-01,  9.1924e-01],\n",
      "        [ 1.2386e+00,  2.3908e+00, -5.4036e-01, -6.2478e-01,  2.0242e+00],\n",
      "        [-4.9764e-01,  1.7458e+00,  1.4515e+00,  1.9962e+00, -2.0386e-01],\n",
      "        [-7.2192e-01, -5.4355e-01,  2.5834e-01,  4.2427e-01, -9.9682e-01],\n",
      "        [-9.4364e-01, -9.8954e-01,  3.6734e-01,  2.7801e-01, -1.8235e-01],\n",
      "        [-4.4732e-01, -1.1936e+00,  4.4933e-02,  1.0485e+00, -1.5466e+00],\n",
      "        [-5.2157e-01,  1.2934e-01, -9.8779e-01,  1.1513e+00, -1.6866e+00],\n",
      "        [ 1.0969e+00,  1.4244e+00, -3.0017e+00, -6.9866e-02,  5.7736e-01],\n",
      "        [-4.7919e-01, -1.6979e+00, -1.2185e+00, -2.3221e+00,  5.7728e-01],\n",
      "        [ 1.0631e+00, -1.6846e-01, -1.2135e+00, -1.3869e+00,  1.7059e+00],\n",
      "        [ 8.7974e-01,  9.1310e-01,  4.8214e-01,  6.6875e-01,  4.2596e-01],\n",
      "        [ 1.7134e-01,  5.8758e-02,  7.4422e-01, -2.6206e-01, -6.2239e-01],\n",
      "        [-2.5928e+00,  1.7714e+00,  1.9045e+00,  3.2952e-01, -9.0626e-01],\n",
      "        [-1.4348e+00,  6.4810e-01, -2.4148e-01, -9.9878e-02,  1.0256e+00],\n",
      "        [ 2.5811e-01,  1.6606e+00, -1.2204e+00,  1.6596e+00,  8.4913e-01],\n",
      "        [ 1.4445e-02,  9.8988e-01,  7.6966e-01,  1.1989e+00,  4.4329e-01],\n",
      "        [-1.2278e+00,  7.0728e-01,  5.8842e-01,  1.4312e-01, -9.1652e-01],\n",
      "        [-4.5068e-01,  9.2653e-01, -1.8329e-01,  3.4323e-01, -6.4612e-01],\n",
      "        [-3.7011e-01, -1.3320e+00,  6.3624e-01, -3.8484e-01,  1.2911e+00],\n",
      "        [ 1.8179e-02, -1.9797e+00, -4.8684e-01, -2.3537e-01, -1.6609e+00],\n",
      "        [-9.7500e-01,  1.1752e+00, -3.4447e-01,  4.5569e-01,  1.2014e+00],\n",
      "        [ 6.6850e-01, -1.0411e-01,  1.4355e+00,  1.2171e+00,  1.5394e+00],\n",
      "        [ 8.3988e-01,  6.9243e-01, -1.0610e+00,  5.7873e-01,  1.4887e+00],\n",
      "        [-1.2425e+00,  4.3352e-01,  8.0342e-01,  8.8476e-01, -5.4928e-01],\n",
      "        [-6.3651e-01, -1.6809e+00, -6.6791e-01,  4.9589e-01,  3.0839e-01],\n",
      "        [ 7.1698e-01, -6.8309e-01,  1.9426e+00, -1.0391e+00, -5.0698e-01],\n",
      "        [-2.3752e-01,  2.6928e-01,  9.5290e-01,  5.3686e-01,  4.0734e-01],\n",
      "        [ 3.3572e-01, -6.2058e-01, -1.4280e+00,  1.1268e+00, -9.2143e-02],\n",
      "        [-4.4228e-01, -1.2476e+00,  6.9672e-01,  1.1913e+00, -1.2355e+00],\n",
      "        [-1.8451e-01,  3.6493e-03,  2.2216e-01,  6.4290e-01, -1.2707e+00],\n",
      "        [-3.5371e-01, -1.2586e+00,  2.7531e-01,  7.1306e-01,  1.1764e+00],\n",
      "        [ 7.3264e-01, -5.1754e-01, -1.4263e-01,  2.8608e-01,  6.2394e-01],\n",
      "        [ 1.9702e-01, -1.9957e-01, -2.4100e-01,  9.8016e-01, -1.1478e+00],\n",
      "        [-4.8091e-01,  1.3505e-01,  1.2922e-01,  7.6471e-02, -4.4061e-01],\n",
      "        [-7.5049e-01, -4.9846e-01, -3.8839e+00,  8.6256e-01,  8.2824e-01],\n",
      "        [ 2.8101e-01, -2.2909e-02, -6.6839e-02, -5.6241e-01, -3.4951e-01]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2911],\n",
       "        [ 0.1600],\n",
       "        [ 1.1809],\n",
       "        [-2.8450],\n",
       "        [-0.3903],\n",
       "        [-1.2372],\n",
       "        [ 1.0634],\n",
       "        [-3.0921],\n",
       "        [-0.5314],\n",
       "        [ 0.0776],\n",
       "        [-1.1601],\n",
       "        [-0.3069],\n",
       "        [-0.3988],\n",
       "        [ 2.6923],\n",
       "        [ 0.7647],\n",
       "        [-0.9587],\n",
       "        [-1.4277],\n",
       "        [-1.4491],\n",
       "        [ 0.3982],\n",
       "        [-0.0723],\n",
       "        [ 2.3588],\n",
       "        [ 1.4993],\n",
       "        [ 0.2594],\n",
       "        [-1.2482],\n",
       "        [-0.6160],\n",
       "        [ 0.2159],\n",
       "        [-0.2933],\n",
       "        [-0.8725],\n",
       "        [-0.3332],\n",
       "        [ 1.9830],\n",
       "        [ 1.2885],\n",
       "        [-0.4255],\n",
       "        [-1.3260],\n",
       "        [-1.8426],\n",
       "        [-0.0525],\n",
       "        [-0.7985]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_data = torch.randn(36, 1)\n",
    "\n",
    "en = nn.Embedding(36, 5)\n",
    "inds = torch.LongTensor([0, 1, 2])\n",
    "print(list(en.parameters()))\n",
    "\n",
    "en(inds)\n",
    "rand_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e117da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-2.1799, -0.6837,  0.9874],\n",
      "        [ 0.1200, -0.5580, -0.9353],\n",
      "        [ 0.0599, -0.9523, -0.5982],\n",
      "        [-0.9337, -1.3064, -1.2706],\n",
      "        [-0.6115, -0.0693,  0.7232]], requires_grad=True)]\n",
      "tensor([[ 0.1200, -0.5580, -0.9353],\n",
      "        [ 0.0599, -0.9523, -0.5982]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 0.1200, -0.5580, -0.9353,  0.0599, -0.9523, -0.5982]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "te = nn.Embedding(5, 3)\n",
    "\n",
    "emidxs = torch.tensor([1, 2])\n",
    "\n",
    "entire = list(te.parameters())\n",
    "\n",
    "vanilla = te(emidxs)\n",
    "\n",
    "fancy = te(emidxs).view((1, -1))\n",
    "\n",
    "print(\"{}\\n{}\\n{}\".format(entire, vanilla, fancy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c07b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "print(ngrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e68f645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: winters  |  context: ['forty', 'When']\n",
      "tensor([12, 89])\n",
      "target: shall  |  context: ['winters', 'forty']\n",
      "tensor([35, 12])\n",
      "target: besiege  |  context: ['shall', 'winters']\n",
      "tensor([13, 35])\n",
      "target: thy  |  context: ['besiege', 'shall']\n",
      "tensor([69, 13])\n",
      "target: brow,  |  context: ['thy', 'besiege']\n",
      "tensor([50, 69])\n",
      "target: And  |  context: ['brow,', 'thy']\n",
      "tensor([51, 50])\n",
      "target: dig  |  context: ['And', 'brow,']\n",
      "tensor([46, 51])\n",
      "target: deep  |  context: ['dig', 'And']\n",
      "tensor([ 6, 46])\n",
      "target: trenches  |  context: ['deep', 'dig']\n",
      "tensor([52,  6])\n",
      "target: in  |  context: ['trenches', 'deep']\n",
      "tensor([34, 52])\n",
      "target: thy  |  context: ['in', 'trenches']\n",
      "tensor([78, 34])\n",
      "target: beauty's  |  context: ['thy', 'in']\n",
      "tensor([50, 78])\n",
      "target: field,  |  context: [\"beauty's\", 'thy']\n",
      "tensor([37, 50])\n",
      "target: Thy  |  context: ['field,', \"beauty's\"]\n",
      "tensor([85, 37])\n",
      "target: youth's  |  context: ['Thy', 'field,']\n",
      "tensor([36, 85])\n",
      "target: proud  |  context: [\"youth's\", 'Thy']\n",
      "tensor([83, 36])\n",
      "target: livery  |  context: ['proud', \"youth's\"]\n",
      "tensor([27, 83])\n",
      "target: so  |  context: ['livery', 'proud']\n",
      "tensor([92, 27])\n",
      "target: gazed  |  context: ['so', 'livery']\n",
      "tensor([30, 92])\n",
      "target: on  |  context: ['gazed', 'so']\n",
      "tensor([16, 30])\n",
      "target: now,  |  context: ['on', 'gazed']\n",
      "tensor([55, 16])\n",
      "target: Will  |  context: ['now,', 'on']\n",
      "tensor([38, 55])\n",
      "target: be  |  context: ['Will', 'now,']\n",
      "tensor([70, 38])\n",
      "target: a  |  context: ['be', 'Will']\n",
      "tensor([66, 70])\n",
      "target: totter'd  |  context: ['a', 'be']\n",
      "tensor([24, 66])\n",
      "target: weed  |  context: [\"totter'd\", 'a']\n",
      "tensor([39, 24])\n",
      "target: of  |  context: ['weed', \"totter'd\"]\n",
      "tensor([ 1, 39])\n",
      "target: small  |  context: ['of', 'weed']\n",
      "tensor([42,  1])\n",
      "target: worth  |  context: ['small', 'of']\n",
      "tensor([28, 42])\n",
      "target: held:  |  context: ['worth', 'small']\n",
      "tensor([76, 28])\n",
      "target: Then  |  context: ['held:', 'worth']\n",
      "tensor([48, 76])\n",
      "target: being  |  context: ['Then', 'held:']\n",
      "tensor([58, 48])\n",
      "target: asked,  |  context: ['being', 'Then']\n",
      "tensor([ 4, 58])\n",
      "target: where  |  context: ['asked,', 'being']\n",
      "tensor([72,  4])\n",
      "target: all  |  context: ['where', 'asked,']\n",
      "tensor([41, 72])\n",
      "target: thy  |  context: ['all', 'where']\n",
      "tensor([47, 41])\n",
      "target: beauty  |  context: ['thy', 'all']\n",
      "tensor([50, 47])\n",
      "target: lies,  |  context: ['beauty', 'thy']\n",
      "tensor([ 8, 50])\n",
      "target: Where  |  context: ['lies,', 'beauty']\n",
      "tensor([0, 8])\n",
      "target: all  |  context: ['Where', 'lies,']\n",
      "tensor([26,  0])\n",
      "target: the  |  context: ['all', 'Where']\n",
      "tensor([47, 26])\n",
      "target: treasure  |  context: ['the', 'all']\n",
      "tensor([67, 47])\n",
      "target: of  |  context: ['treasure', 'the']\n",
      "tensor([21, 67])\n",
      "target: thy  |  context: ['of', 'treasure']\n",
      "tensor([42, 21])\n",
      "target: lusty  |  context: ['thy', 'of']\n",
      "tensor([50, 42])\n",
      "target: days;  |  context: ['lusty', 'thy']\n",
      "tensor([20, 50])\n",
      "target: To  |  context: ['days;', 'lusty']\n",
      "tensor([91, 20])\n",
      "target: say,  |  context: ['To', 'days;']\n",
      "tensor([95, 91])\n",
      "target: within  |  context: ['say,', 'To']\n",
      "tensor([93, 95])\n",
      "target: thine  |  context: ['within', 'say,']\n",
      "tensor([25, 93])\n",
      "target: own  |  context: ['thine', 'within']\n",
      "tensor([68, 25])\n",
      "target: deep  |  context: ['own', 'thine']\n",
      "tensor([18, 68])\n",
      "target: sunken  |  context: ['deep', 'own']\n",
      "tensor([52, 18])\n",
      "target: eyes,  |  context: ['sunken', 'deep']\n",
      "tensor([56, 52])\n",
      "target: Were  |  context: ['eyes,', 'sunken']\n",
      "tensor([80, 56])\n",
      "target: an  |  context: ['Were', 'eyes,']\n",
      "tensor([43, 80])\n",
      "target: all-eating  |  context: ['an', 'Were']\n",
      "tensor([62, 43])\n",
      "target: shame,  |  context: ['all-eating', 'an']\n",
      "tensor([ 2, 62])\n",
      "target: and  |  context: ['shame,', 'all-eating']\n",
      "tensor([54,  2])\n",
      "target: thriftless  |  context: ['and', 'shame,']\n",
      "tensor([63, 54])\n",
      "target: praise.  |  context: ['thriftless', 'and']\n",
      "tensor([74, 63])\n",
      "target: How  |  context: ['praise.', 'thriftless']\n",
      "tensor([71, 74])\n",
      "target: much  |  context: ['How', 'praise.']\n",
      "tensor([40, 71])\n",
      "target: more  |  context: ['much', 'How']\n",
      "tensor([19, 40])\n",
      "target: praise  |  context: ['more', 'much']\n",
      "tensor([82, 19])\n",
      "target: deserv'd  |  context: ['praise', 'more']\n",
      "tensor([75, 82])\n",
      "target: thy  |  context: [\"deserv'd\", 'praise']\n",
      "tensor([84, 75])\n",
      "target: beauty's  |  context: ['thy', \"deserv'd\"]\n",
      "tensor([50, 84])\n",
      "target: use,  |  context: [\"beauty's\", 'thy']\n",
      "tensor([37, 50])\n",
      "target: If  |  context: ['use,', \"beauty's\"]\n",
      "tensor([45, 37])\n",
      "target: thou  |  context: ['If', 'use,']\n",
      "tensor([17, 45])\n",
      "target: couldst  |  context: ['thou', 'If']\n",
      "tensor([60, 17])\n",
      "target: answer  |  context: ['couldst', 'thou']\n",
      "tensor([10, 60])\n",
      "target: 'This  |  context: ['answer', 'couldst']\n",
      "tensor([90, 10])\n",
      "target: fair  |  context: [\"'This\", 'answer']\n",
      "tensor([29, 90])\n",
      "target: child  |  context: ['fair', \"'This\"]\n",
      "tensor([44, 29])\n",
      "target: of  |  context: ['child', 'fair']\n",
      "tensor([49, 44])\n",
      "target: mine  |  context: ['of', 'child']\n",
      "tensor([42, 49])\n",
      "target: Shall  |  context: ['mine', 'of']\n",
      "tensor([ 9, 42])\n",
      "target: sum  |  context: ['Shall', 'mine']\n",
      "tensor([86,  9])\n",
      "target: my  |  context: ['sum', 'Shall']\n",
      "tensor([61, 86])\n",
      "target: count,  |  context: ['my', 'sum']\n",
      "tensor([33, 61])\n",
      "target: and  |  context: ['count,', 'my']\n",
      "tensor([53, 33])\n",
      "target: make  |  context: ['and', 'count,']\n",
      "tensor([63, 53])\n",
      "target: my  |  context: ['make', 'and']\n",
      "tensor([65, 63])\n",
      "target: old  |  context: ['my', 'make']\n",
      "tensor([33, 65])\n",
      "target: excuse,'  |  context: ['old', 'my']\n",
      "tensor([96, 33])\n",
      "target: Proving  |  context: [\"excuse,'\", 'old']\n",
      "tensor([15, 96])\n",
      "target: his  |  context: ['Proving', \"excuse,'\"]\n",
      "tensor([14, 15])\n",
      "target: beauty  |  context: ['his', 'Proving']\n",
      "tensor([73, 14])\n",
      "target: by  |  context: ['beauty', 'his']\n",
      "tensor([ 8, 73])\n",
      "target: succession  |  context: ['by', 'beauty']\n",
      "tensor([94,  8])\n",
      "target: thine!  |  context: ['succession', 'by']\n",
      "tensor([88, 94])\n",
      "target: This  |  context: ['thine!', 'succession']\n",
      "tensor([64, 88])\n",
      "target: were  |  context: ['This', 'thine!']\n",
      "tensor([ 5, 64])\n",
      "target: to  |  context: ['were', 'This']\n",
      "tensor([77,  5])\n",
      "target: be  |  context: ['to', 'were']\n",
      "tensor([87, 77])\n",
      "target: new  |  context: ['be', 'to']\n",
      "tensor([66, 87])\n",
      "target: made  |  context: ['new', 'be']\n",
      "tensor([22, 66])\n",
      "target: when  |  context: ['made', 'new']\n",
      "tensor([79, 22])\n",
      "target: thou  |  context: ['when', 'made']\n",
      "tensor([ 3, 79])\n",
      "target: art  |  context: ['thou', 'when']\n",
      "tensor([60,  3])\n",
      "target: old,  |  context: ['art', 'thou']\n",
      "tensor([11, 60])\n",
      "target: And  |  context: ['old,', 'art']\n",
      "tensor([59, 11])\n",
      "target: see  |  context: ['And', 'old,']\n",
      "tensor([46, 59])\n",
      "target: thy  |  context: ['see', 'And']\n",
      "tensor([ 7, 46])\n",
      "target: blood  |  context: ['thy', 'see']\n",
      "tensor([50,  7])\n",
      "target: warm  |  context: ['blood', 'thy']\n",
      "tensor([32, 50])\n",
      "target: when  |  context: ['warm', 'blood']\n",
      "tensor([23, 32])\n",
      "target: thou  |  context: ['when', 'warm']\n",
      "tensor([ 3, 23])\n",
      "target: feel'st  |  context: ['thou', 'when']\n",
      "tensor([60,  3])\n",
      "target: it  |  context: [\"feel'st\", 'thou']\n",
      "tensor([81, 60])\n",
      "target: cold.  |  context: ['it', \"feel'st\"]\n",
      "tensor([31, 81])\n"
     ]
    }
   ],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "for context, target in ngrams:\n",
    "    print(\"target: {}  |  context: {}\".format(target, context))\n",
    "    context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "    print(context_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92cc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
