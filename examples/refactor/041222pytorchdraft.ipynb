{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213bcdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.distributions import Multinomial\n",
    "import biom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382bb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some example data\n",
    "microbes = biom.load_table(\"./soil_microbes.biom\")\n",
    "metabolites = biom.load_table(\"./soil_metabolites.biom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96fac3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicrobeMetaboliteData(Dataset):\n",
    "    def __init__(self, microbes: biom.table, metabolites: biom.table):\n",
    "        # arrange\n",
    "        self.microbes = microbes.to_dataframe().T   \n",
    "        self.metabolites = metabolites.to_dataframe().T\n",
    "        \n",
    "        # only samples that have results\n",
    "        self.microbes = self.microbes.loc[self.metabolites.index]\n",
    "      \n",
    "        # convert to tensors/final form\n",
    "        self.microbes = torch.tensor(self.microbes.values, dtype=torch.int)\n",
    "        self.metabolites = torch.tensor(self.metabolites.values, dtype=torch.int64)\n",
    "        \n",
    "        # counts\n",
    "        self.microbe_count = self.microbes.shape[1]\n",
    "        self.metabolite_count = self.metabolites.shape[1]\n",
    "        \n",
    "        # relative frequencies\n",
    "        self.microbe_relative_frequency = (self.microbes.T\n",
    "                                      / self.microbes.sum(1)\n",
    "                                     ).T\n",
    "        \n",
    "        self.metabolite_relative_frequency = (self.metabolites.T\n",
    "                                     / self.metabolites.sum(1)\n",
    "                                    ).T\n",
    "        \n",
    "        self.total_microbe_observations = self.microbes.sum()\n",
    "       \n",
    "    def __len__(self):\n",
    "        return self.total_microbe_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234ccc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = MicrobeMetaboliteData(microbes, metabolites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab12e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424846"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.total_microbe_observations.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f106a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMVec(nn.Module):\n",
    "    def __init__(self, num_microbes, num_metabolites, latent_dim):\n",
    "        super().__init__()\n",
    "        #\n",
    "        self.encoder = nn.Embedding(num_microbes, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, num_metabolites),\n",
    "            # [batch, sample, metabolite]\n",
    "            nn.Softmax(dim=2)\n",
    "        )\n",
    "        \n",
    "    # X = batch_size of microbe indexes\n",
    "    # Y = expected metabolite data\n",
    "    def forward(self, X, Y):\n",
    "        \n",
    "        # pass our random draws to our embedding\n",
    "        z = self.encoder(X)\n",
    "        \n",
    "        # from latent dimensions in embedding through\n",
    "        # our linear function to predicted metabolite frequencies which\n",
    "        # we then normalize with softmax\n",
    "        y_pred = self.decoder(z)\n",
    "        \n",
    "        # total_count=0 and validate_args=False allows skipping total count when calling log_prob\n",
    "        # as there having floating point issues leading to \"incorrect\" total counts.\n",
    "        # This multinomial is generated from the output of the single\n",
    "        forward_dist = Multinomial(total_count=0,\n",
    "                                  validate_args=False,\n",
    "                                  probs=y_pred)\n",
    "        \n",
    "        # the log probability of drawing our expected results from our \"predictions\"\n",
    "        forward_dist = forward_dist.log_prob(Y)\n",
    "        \n",
    "        # get sample loss, a sample in each \"row\"/ zeroeth dimension of the tensor\n",
    "        forward_dist = forward_dist.mean(0)\n",
    "        \n",
    "        # total log probability loss in regards to all samples\n",
    "        lp = forward_dist.mean()\n",
    "\n",
    "        return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74bdf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmvec_model = MMVec(example_data.microbe_count, example_data.metabolite_count, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc8d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataset, model, optimizer, batch_size):\n",
    "    \n",
    "    # because we are wanting to look at all of the samples together we are having to \n",
    "    # handle our own batching for now. This method currently leads to slight over-\n",
    "    # sampling but can be refined.\n",
    "    n_batches = torch.div(dataset.total_microbe_observations.item(),\n",
    "                          batch_size,\n",
    "                          rounding_mode = 'floor') + 1\n",
    "    \n",
    "    # We will want to implement batching functionality later for\n",
    "    # paralizability<tm>, but for now running on cpu this works.\n",
    "    for batch in range(n_batches * epochs):\n",
    "        \n",
    "        # the draws we will be training each batch on that will\n",
    "        # be fed to all samples in our model. This step will probably be\n",
    "        # moved to a sampler or collate_fn somewhere in the dataset/dataloader\n",
    "        # but how exactly that will work is not clear at the moment\n",
    "        draws = torch.multinomial(dataset.microbe_relative_frequency,\n",
    "                                  batch_size,\n",
    "                                  replacement=True).T\n",
    "        \n",
    "        # \"forward step\", our model generates our \"predictions\", so there is no need to\n",
    "        # call `forward` separately.\n",
    "        lp = model(draws,\n",
    "                   dataset.metabolite_relative_frequency)\n",
    "        \n",
    "        # this location is idiomatic but flexible\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # the typical training bit.\n",
    "        lp.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f\"loss: {lp.item()}\\nBatch #: {batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb75b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -4.114527225494385\n",
      "Batch #: 0\n",
      "loss: -3.6144325733184814\n",
      "Batch #: 100\n",
      "loss: -3.0469698905944824\n",
      "Batch #: 200\n",
      "loss: -2.70939564704895\n",
      "Batch #: 300\n",
      "loss: -2.5499744415283203\n",
      "Batch #: 400\n",
      "loss: -2.473045587539673\n",
      "Batch #: 500\n",
      "loss: -2.4374732971191406\n",
      "Batch #: 600\n",
      "loss: -2.421781539916992\n",
      "Batch #: 700\n",
      "loss: -2.4101920127868652\n",
      "Batch #: 800\n",
      "loss: -2.4041030406951904\n",
      "Batch #: 900\n",
      "loss: -2.4012131690979004\n",
      "Batch #: 1000\n",
      "loss: -2.397974967956543\n",
      "Batch #: 1100\n",
      "loss: -2.3931915760040283\n",
      "Batch #: 1200\n",
      "loss: -2.3923048973083496\n",
      "Batch #: 1300\n",
      "loss: -2.389982223510742\n",
      "Batch #: 1400\n",
      "loss: -2.3868303298950195\n",
      "Batch #: 1500\n",
      "loss: -2.3855628967285156\n",
      "Batch #: 1600\n",
      "loss: -2.382643222808838\n",
      "Batch #: 1700\n",
      "loss: -2.381664991378784\n",
      "Batch #: 1800\n",
      "loss: -2.3774473667144775\n",
      "Batch #: 1900\n",
      "loss: -2.378610372543335\n",
      "Batch #: 2000\n",
      "loss: -2.3776485919952393\n",
      "Batch #: 2100\n",
      "loss: -2.376375675201416\n",
      "Batch #: 2200\n",
      "loss: -2.3723671436309814\n",
      "Batch #: 2300\n",
      "loss: -2.372851848602295\n",
      "Batch #: 2400\n",
      "loss: -2.373134136199951\n",
      "Batch #: 2500\n",
      "loss: -2.3704051971435547\n",
      "Batch #: 2600\n",
      "loss: -2.37052059173584\n",
      "Batch #: 2700\n",
      "loss: -2.371293306350708\n",
      "Batch #: 2800\n",
      "loss: -2.3711659908294678\n",
      "Batch #: 2900\n",
      "loss: -2.3693435192108154\n",
      "Batch #: 3000\n",
      "loss: -2.370833396911621\n",
      "Batch #: 3100\n",
      "loss: -2.36956787109375\n",
      "Batch #: 3200\n",
      "loss: -2.3683981895446777\n",
      "Batch #: 3300\n",
      "loss: -2.368025064468384\n",
      "Batch #: 3400\n",
      "loss: -2.3673665523529053\n",
      "Batch #: 3500\n",
      "loss: -2.3669538497924805\n",
      "Batch #: 3600\n",
      "loss: -2.364877700805664\n",
      "Batch #: 3700\n",
      "loss: -2.3676393032073975\n",
      "Batch #: 3800\n",
      "loss: -2.3655707836151123\n",
      "Batch #: 3900\n",
      "loss: -2.365952253341675\n",
      "Batch #: 4000\n",
      "loss: -2.366527557373047\n",
      "Batch #: 4100\n",
      "loss: -2.364421844482422\n",
      "Batch #: 4200\n",
      "loss: -2.363978385925293\n",
      "Batch #: 4300\n",
      "loss: -2.3649704456329346\n",
      "Batch #: 4400\n",
      "loss: -2.364382743835449\n",
      "Batch #: 4500\n",
      "loss: -2.361299991607666\n",
      "Batch #: 4600\n",
      "loss: -2.3609752655029297\n",
      "Batch #: 4700\n",
      "loss: -2.3623459339141846\n",
      "Batch #: 4800\n",
      "loss: -2.3606176376342773\n",
      "Batch #: 4900\n",
      "loss: -2.3621227741241455\n",
      "Batch #: 5000\n",
      "loss: -2.3601856231689453\n",
      "Batch #: 5100\n",
      "loss: -2.3616325855255127\n",
      "Batch #: 5200\n",
      "loss: -2.3607864379882812\n",
      "Batch #: 5300\n",
      "loss: -2.3603267669677734\n",
      "Batch #: 5400\n",
      "loss: -2.3611979484558105\n",
      "Batch #: 5500\n",
      "loss: -2.36138653755188\n",
      "Batch #: 5600\n",
      "loss: -2.3617565631866455\n",
      "Batch #: 5700\n",
      "loss: -2.3602635860443115\n",
      "Batch #: 5800\n",
      "loss: -2.3588624000549316\n",
      "Batch #: 5900\n",
      "loss: -2.363048791885376\n",
      "Batch #: 6000\n",
      "loss: -2.357430934906006\n",
      "Batch #: 6100\n",
      "loss: -2.359692335128784\n",
      "Batch #: 6200\n",
      "loss: -2.359476327896118\n",
      "Batch #: 6300\n",
      "loss: -2.358708381652832\n",
      "Batch #: 6400\n",
      "loss: -2.3578848838806152\n",
      "Batch #: 6500\n",
      "loss: -2.3591620922088623\n",
      "Batch #: 6600\n",
      "loss: -2.3596458435058594\n",
      "Batch #: 6700\n",
      "loss: -2.358290672302246\n",
      "Batch #: 6800\n",
      "loss: -2.3569066524505615\n",
      "Batch #: 6900\n",
      "loss: -2.3586177825927734\n",
      "Batch #: 7000\n",
      "loss: -2.359415054321289\n",
      "Batch #: 7100\n",
      "loss: -2.358649969100952\n",
      "Batch #: 7200\n",
      "loss: -2.35966420173645\n",
      "Batch #: 7300\n",
      "loss: -2.358867883682251\n",
      "Batch #: 7400\n",
      "loss: -2.3568341732025146\n",
      "Batch #: 7500\n",
      "loss: -2.3596749305725098\n",
      "Batch #: 7600\n",
      "loss: -2.359412670135498\n",
      "Batch #: 7700\n",
      "loss: -2.357198476791382\n",
      "Batch #: 7800\n",
      "loss: -2.358001947402954\n",
      "Batch #: 7900\n",
      "loss: -2.3569891452789307\n",
      "Batch #: 8000\n",
      "loss: -2.3587193489074707\n",
      "Batch #: 8100\n",
      "loss: -2.3581130504608154\n",
      "Batch #: 8200\n",
      "loss: -2.3578381538391113\n",
      "Batch #: 8300\n",
      "loss: -2.357231855392456\n",
      "Batch #: 8400\n",
      "loss: -2.3578529357910156\n",
      "Batch #: 8500\n",
      "loss: -2.3557262420654297\n",
      "Batch #: 8600\n",
      "loss: -2.355126142501831\n",
      "Batch #: 8700\n",
      "loss: -2.3567700386047363\n",
      "Batch #: 8800\n",
      "loss: -2.3553476333618164\n",
      "Batch #: 8900\n",
      "loss: -2.356520175933838\n",
      "Batch #: 9000\n",
      "loss: -2.3572936058044434\n",
      "Batch #: 9100\n",
      "loss: -2.358710527420044\n",
      "Batch #: 9200\n",
      "loss: -2.3547816276550293\n",
      "Batch #: 9300\n",
      "loss: -2.3565027713775635\n",
      "Batch #: 9400\n",
      "loss: -2.3561108112335205\n",
      "Batch #: 9500\n",
      "loss: -2.356635808944702\n",
      "Batch #: 9600\n",
      "loss: -2.356121301651001\n",
      "Batch #: 9700\n",
      "loss: -2.3586411476135254\n",
      "Batch #: 9800\n",
      "loss: -2.3572912216186523\n",
      "Batch #: 9900\n",
      "loss: -2.35567045211792\n",
      "Batch #: 10000\n",
      "loss: -2.3584144115448\n",
      "Batch #: 10100\n",
      "loss: -2.3562276363372803\n",
      "Batch #: 10200\n",
      "loss: -2.3546085357666016\n",
      "Batch #: 10300\n",
      "loss: -2.3559350967407227\n",
      "Batch #: 10400\n",
      "loss: -2.356455087661743\n",
      "Batch #: 10500\n",
      "loss: -2.3574140071868896\n",
      "Batch #: 10600\n",
      "loss: -2.3562002182006836\n",
      "Batch #: 10700\n",
      "loss: -2.35746169090271\n",
      "Batch #: 10800\n",
      "loss: -2.3548736572265625\n",
      "Batch #: 10900\n",
      "loss: -2.3564090728759766\n",
      "Batch #: 11000\n",
      "loss: -2.3564658164978027\n",
      "Batch #: 11100\n",
      "loss: -2.3554699420928955\n",
      "Batch #: 11200\n",
      "loss: -2.3563244342803955\n",
      "Batch #: 11300\n",
      "loss: -2.357598066329956\n",
      "Batch #: 11400\n",
      "loss: -2.35477614402771\n",
      "Batch #: 11500\n",
      "loss: -2.3572442531585693\n",
      "Batch #: 11600\n",
      "loss: -2.357273817062378\n",
      "Batch #: 11700\n",
      "loss: -2.3560562133789062\n",
      "Batch #: 11800\n",
      "loss: -2.355698823928833\n",
      "Batch #: 11900\n",
      "loss: -2.3559463024139404\n",
      "Batch #: 12000\n",
      "loss: -2.35664439201355\n",
      "Batch #: 12100\n",
      "loss: -2.355379104614258\n",
      "Batch #: 12200\n",
      "loss: -2.354964256286621\n",
      "Batch #: 12300\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 500\n",
    "epochs = 25\n",
    "optimizer = torch.optim.Adam(mmvec_model.parameters(), lr=learning_rate, maximize=True)\n",
    "\n",
    "# run the training loop    \n",
    "train_loop(dataset=example_data, model=mmvec_model, optimizer=optimizer, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
